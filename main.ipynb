{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d692fe-9107-4de8-b104-63e8ca141b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain openai faiss-cpu tiktoken\n",
    "import os\n",
    "import pandas as pd\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...gMwA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b3700-2ed1-467f-904d-a359964a2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2df86-15e2-4ed3-9034-ea5ef1b018fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebe184-6f96-4ed3-89ca-eca3451eb6f1",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab24d5d-ce49-44dc-bdbe-8ab7075c2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Question': [\n",
    "        'What is Python?',\n",
    "        'What is AI?',\n",
    "        'Define Machine Learning',\n",
    "        'Explain Deep Learning'\n",
    "    ],\n",
    "    'Answer': [\n",
    "        'Python is a programming language.',\n",
    "        'AI stands for Artificial Intelligence.',\n",
    "        'Machine Learning is a subset of AI that involves training algorithms.',\n",
    "        'Deep Learning is a subset of ML that uses neural networks.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"faq_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cc386-0d31-4e5c-8565-3326cbcaaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load data from CSV\n",
    "loader = CSVLoader(file_path='faq_data.csv')\n",
    "documents = loader.load()\n",
    "\n",
    "# Optional: Split into chunks if long\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# Check a sample\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e24230-e835-44fb-b684-05a46be19eb6",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0938b938-c90e-4903-bf5f-c0629fd0b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaibh\\anaconda3\\envs\\py310_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\vaibh\\AppData\\Local\\Temp\\ipykernel_4084\\620008815.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "Device set to use cpu\n",
      "C:\\Users\\vaibh\\AppData\\Local\\Temp\\ipykernel_4084\\620008815.py:36: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a subset of AI that involves training algorithms\n"
     ]
    }
   ],
   "source": [
    " # !pip install langchain langchain-community sentence-transformers transformers faiss-cpu huggingface-hub\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load documents\n",
    "loader = TextLoader(\"faq_data.csv\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Generate embeddings using Hugging Face\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vector store\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Set up retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Load HF QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", \n",
    "                       tokenizer=\"deepset/roberta-base-squad2\", \n",
    "                       use_auth_token=\"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\")\n",
    "\n",
    "# Ask question\n",
    "query = \"What is Machine Learning?\"\n",
    "\n",
    "# Retrieve relevant context\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "context = \" \".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "# Run QA pipeline\n",
    "result = qa_pipeline(question=query, context=context)\n",
    "\n",
    "# Output result\n",
    "print(\"Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6454dd7-9e78-4f6a-ae76-337ce2dbee23",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b7c53-2654-4467-bee3-407610807ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01f023-a91c-473b-a072-8c44f13ef02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2701e2d-652a-4773-9325-732c645faf10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310_env]",
   "language": "python",
   "name": "conda-env-py310_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
